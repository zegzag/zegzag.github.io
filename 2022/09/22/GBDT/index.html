<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="GBDT,XGBoost,LightGBM,LTR,NDCG," />










<meta name="description" content="这篇文档系统的总结经典树模型 (Decision Tree)，经典梯度提升树模型 (GBDT, XGBoost, LightGBM)，以及其在 LTR (Learning To Rank) 上的应用(LambdaMART)。旨在理解业界著名的开源库 XGBoost 和 LightGBM 背后的原理及其相关参数含义。具备高等数学，线性代数和概率论的基础知识即可全篇读懂。相关业界背景关键">
<meta property="og:type" content="article">
<meta property="og:title" content="从Decision Tree 到 LightGBM 到 LambdaMART">
<meta property="og:url" content="http://example.com/2022/09/22/GBDT/index.html">
<meta property="og:site_name" content="泽">
<meta property="og:description" content="这篇文档系统的总结经典树模型 (Decision Tree)，经典梯度提升树模型 (GBDT, XGBoost, LightGBM)，以及其在 LTR (Learning To Rank) 上的应用(LambdaMART)。旨在理解业界著名的开源库 XGBoost 和 LightGBM 背后的原理及其相关参数含义。具备高等数学，线性代数和概率论的基础知识即可全篇读懂。相关业界背景关键">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/images/xgboost_approximate_rank_function.jpg">
<meta property="article:published_time" content="2022-09-22T12:38:00.000Z">
<meta property="article:modified_time" content="2022-12-18T12:38:00.000Z">
<meta property="article:author" content="Z">
<meta property="article:tag" content="GBDT">
<meta property="article:tag" content="XGBoost">
<meta property="article:tag" content="LightGBM">
<meta property="article:tag" content="LTR">
<meta property="article:tag" content="NDCG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/xgboost_approximate_rank_function.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2022/09/22/GBDT/"/>





  <title>从Decision Tree 到 LightGBM 到 LambdaMART | 泽</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">泽</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">君子藏器于身，待时而动</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/22/GBDT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="泽">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">从Decision Tree 到 LightGBM 到 LambdaMART</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-09-22T20:38:00+08:00">
                2022-09-22
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2022-12-18T20:38:00+08:00">
                2022-12-18
              </time>
            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>   这篇文档系统的总结经典树模型 (Decision Tree)，经典梯度提升树模型
(GBDT, XGBoost, LightGBM)，以及其在 LTR (Learning To Rank)
上的应用(LambdaMART)。旨在理解业界著名的开源库 <code>XGBoost</code> 和
<code>LightGBM</code>
背后的原理及其相关参数含义。具备高等数学，线性代数和概率论的基础知识即可全篇读懂。相关业界背景关键词：<code>搜索</code>，<code>推荐</code>，<code>排序算法</code></p>
<hr />
<h1 id="gbdt-相关算法">GBDT 相关算法</h1>
<h2 id="decision-tree">Decision Tree</h2>
<h3 id="映射表示">映射表示</h3>
<p>  对于寻找 <span class="math inline">\((\mathbf{x}, y),\ x\in R^m,\
y\in R\)</span> (即 <span class="math inline">\(m\)</span> 维特征)
之间的映射关系 <span
class="math inline">\(f\)</span>，决策树进行如下建模： <span
class="math display">\[
    f(\mathbf{x};\mathbf{w}) = tree(\mathbf{x}; \mathbf{w}) =
\sum_{v=1}^V w_v\text{I}(\mathbf{x}\in R^m_v)
\]</span> 其中 <span class="math inline">\(R_j^m\)</span> 是 <span
class="math inline">\(m\)</span> 为空间中的第 <span
class="math inline">\(v\)</span> 块划分，划分方式即树的结构。而 <span
class="math inline">\(w_v\)</span> 则是第 <span
class="math inline">\(v\)</span> 块空间的决策树上的输出值。</p>
<h3 id="基础算法">基础算法</h3>
<p>   摘自西瓜书决策树章节<br />
<img src = "/images/decision_tree_basic_algorithm.png" width=480><br />
注意算法的第 8 步和第 9, 10 步。</p>
<ul>
<li>第 8 步：
<ul>
<li>涉及到如何划分最优属性，即<strong>划分标准</strong>问题</li>
</ul></li>
<li>第 9 步：
<ul>
<li>对于最优属性 <span
class="math inline">\(a_*\)</span>，是划分<strong>多个</strong>子节点，还是划分<strong>两个</strong>子节点。即多叉树或二叉树。涉及到对不同类型的特征的处理问题。特征大致可以包括三类：1.
bool 型特征，只有 0 / 1 两种取值 (二叉划分)。2. float
型特征，即连续性特征，这种可以采用寻找最佳分割节点方式划分(二叉划分)。3.
categorical 型特征，即类别类型特征，这种情况比较麻烦。</li>
</ul></li>
</ul>
<h3 id="id3-算法">ID3 算法</h3>
<ul>
<li>划分标准 <span class="math display">\[
  Ent(D) = -\sum_{k=1}^Kp_klog_2(p_k)\\
  Gain(D, A) = Ent(D) - \sum_{v=1}^V\frac{|D_v|}{|D|}Ent(D_v)
\]</span>
<ul>
<li>划分标准即 <strong>信息增益</strong>：<span
class="math inline">\(Gain\)</span></li>
<li>其中 <span class="math inline">\(K\)</span> 为 <span
class="math inline">\(K\)</span> 个类别, <span
class="math inline">\(V\)</span> 为属性 A 对应的 <span
class="math inline">\(V\)</span> 个划分，<span
class="math inline">\(D_v\)</span> 为属于第 <span
class="math inline">\(v\)</span> 个划分的样本集合</li>
</ul></li>
<li>预测值：
<ul>
<li>划分停止时，该叶子节点内最多的类别为该叶子节点的预测类别。</li>
</ul></li>
</ul>
<h3 id="c4.5-算法">C4.5 算法</h3>
<ul>
<li><p>划分标准：</p>
<p>ID3 算法会倾向于选择类别数较多的属性为最优属性，C4.5算法将
<strong>信息增益</strong> 改为信息增益率。 <span class="math display">\[
  Gain_ratio(D,A) = \frac{Gain(D, A)}{IV(A)}\\
  IV(A) = -\sum_{v=1}^V\frac{|D_v|}{|D|}log_2(\frac{|D_v|}{|D|})
\]</span></p></li>
<li><p>预测值：</p>
<p>划分停止时，该叶子节点内最多的类别为该叶子节点的预测类别。</p></li>
<li><p>如何划分：</p>
<p>针对连续值，将连续的特征离散化。比如m个样本的连续特征A有 <span
class="math inline">\(m\)</span> 个，从小到大排列为 <span
class="math inline">\(a_1,a_2,...,a_m\)</span>,
则C4.5取相邻两样本值的平均数，一共取得 <span
class="math inline">\(m-1\)</span> 划分点，其中第 <span
class="math inline">\(i\)</span>个划分点 <span
class="math inline">\(T_i\)</span>表示为：<span
class="math inline">\(T_i=(a_i+a_i+1)/2\)</span>。对于这 <span
class="math inline">\(m-1\)</span>
个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为at,则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p></li>
<li><p>缺失值处理</p></li>
</ul>
<h3 id="cart-算法">CART 算法</h3>
<ul>
<li><p>CART 分类树：</p>
<ul>
<li>预测值：
<ul>
<li>划分停止时，该叶子节点内最多的类别为该叶子节点的预测类别。</li>
</ul></li>
<li>划分标准： <span class="math display">\[
  Gini(D) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2 =
1-\sum_{k=1}^K(\frac{|D_k|}{|D|})^2\\
\]</span> 假设特征 <span class="math inline">\(A\)</span> 将样本分割成了
<span class="math inline">\(D_1\)</span> 和 <span
class="math inline">\(D_2\)</span> 两部分，则分割后的 <span
class="math inline">\(Gini\)</span> 系数为： <span
class="math display">\[
  Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
\]</span></li>
<li>如何划分：
<ul>
<li><p>对连续值和离散值的处理：</p>
<p>参见：https://www.cnblogs.com/pinard/p/6053344.html</p></li>
<li><p>处理后，依次枚举每个分裂点，选择分裂后 <span
class="math inline">\(Gini\)</span> 系数最小的分裂点</p></li>
</ul></li>
</ul></li>
<li><p>CART 回归树：</p>
<ul>
<li><p>划分标准： <span class="math display">\[
  MSE(D_v) = \sum_{\mathbf{x}_i\in D_v}(y_i - w_i)^2
\]</span> 采用均方差标准，假设特征把样本分成 <span
class="math inline">\(D_1\)</span> 和 <span
class="math inline">\(D_2\)</span> 两部分，则： <span
class="math display">\[
  MSE(D, A) = \sum_{\mathbf{x}_i\in D_1}(y_i - w_i)^2 +
\sum_{\mathbf{x}_i\in D_2}(y_i - w_i)^2
\]</span></p></li>
<li><p>节点预测值：</p>
<p>假设我们已经知道了树的形状，那么每个叶节点的预测值 <span
class="math inline">\(w_v = \overline{y}_v = \frac{\sum_{\mathbf{x}_i\in
D_v} y_i}{|D_v|}\)</span></p></li>
<li><p>如何划分：</p>
<p>按照上诉的标准和节点预测值的计算方式，对连续值和离散值处理同分类 CART
树，依次枚举每个分裂点，选择分裂后 <span
class="math inline">\(MSE\)</span> 最小的分裂点</p></li>
</ul></li>
<li><p>CART 剪枝：</p>
<p>参考：https://www.cnblogs.com/pinard/p/6053344.html</p></li>
</ul>
<hr />
<h2 id="gradient-boosting-gb">Gradient Boosting (GB)</h2>
<h3 id="映射表示-1">映射表示</h3>
<p>  我们有输入样本空间 <span class="math inline">\(D=\{ (\mathbf{x}_i,
y_i)\},\ \ |D|=n,\  \mathbf{x}\in R^m,\ y\in R\)</span>， 即 <span
class="math inline">\(\mathbf{x}\)</span> 有 <span
class="math inline">\(m\)</span> 个 feature, 共 <span
class="math inline">\(n\)</span> 个样本。我们要学习一个复杂的映射 <span
class="math inline">\(F(\mathbf{x})\)</span>，如何学习呢？集成模型的建模思路如下：
<span class="math display">\[
    \hat{y} = F_K(\mathbf{x}) = \sum_{k=1}^K\rho_k f_k(\mathbf{x}) =
F_{K-1}(\mathbf{x}) + \rho_kf_k(\mathbf{x})
\]</span> 即 <span class="math inline">\(F(\mathbf{x})\)</span>
由一系列弱学习器 <span class="math inline">\(f_k(\mathbf{x})\)</span>
组成，<span class="math inline">\(\rho_k\)</span>
为每个弱学习器的权重，而 <span class="math inline">\(f_k\)</span>
一般是一些参数化的模型 (<code>parameterized function</code>)
来表示，比如神经网络，SVM，决策树等。我们表示为 <span
class="math inline">\(f_k(\mathbf{x};
\mathbf{w})\)</span>。损失可以表示为： <span class="math display">\[
    L = \sum_{i=1}^nl(y_i, F_k(\mathbf{x})) = \sum_{i=1}^nl(y_i,
F_{k-1}(\mathbf{x}) + \rho_kf_k(\mathbf{x};\mathbf{w}))
\]</span> 当函数形式 (<code>function</code>) 确定时，确定了<span
class="math inline">\(\mathbf{w}_k\)</span> 可以唯一确定一个映射<span
class="math inline">\(f_k\)</span>，则函数拟合
(<code>function estimation</code>)问题可以转化成参数拟合
(<code>parameter estimation</code>)问题。</p>
<h3 id="学习过程-gradient-boosting">学习过程 (Gradient Boosting)</h3>
<p>  可以看到，损失函数中只有两类未知值：<span
class="math inline">\(\rho, \
\mathbf{w}\)</span>。假设我们已经迭代到了第 <span
class="math inline">\(k\)</span> 步，那么第 <span
class="math inline">\(k\)</span> 步的两类未知值的计算方法如下： <span
class="math display">\[
    \mathbf{w}_k = \underset{\mathbf{w}_k}{\text{arg min}} \sum_{i=1}^n
[g_i-\beta f(\mathbf{x}_k;\mathbf{w})]^2\\
    g_i = -\frac{\partial l(y_i, F_{m-1}(\mathbf{x}_i))}{\partial
F_{m-1}(\mathbf{x}_i)}\\
    \rho_k = \underset{\rho}{\text{arg min}}\sum_{i=1}^nl(y_i,
F_{k-1}(\mathbf{x})+\rho f_k(\mathbf{x};\mathbf{w}_k)) \\
\]</span> 得到第 <span class="math inline">\(k\)</span>
步的模型为：<span
class="math inline">\(F_k(\mathbf{x})=F_{k-1}(\mathbf{x})+\rho_k
f_k(\mathbf{x};\mathbf{w}_k)\)</span>。<br />
  可以看到，第 <span class="math inline">\(k\)</span> 步的弱学习器 <span
class="math inline">\(f_k\)</span> 拟合的是损失函数 <span
class="math inline">\(l\)</span> 在 <span
class="math inline">\(F_{k-1}\)</span>的负梯度 （怎么感觉有点 EM
的味道)。<br />
GB算法： <span class="math display">\[
\begin{aligned}
&amp;F_0(\mathbf{x}) = \underset{\rho}{\text{arg min}} \sum_{i=1}^n
l(y_i, \rho)\\
&amp;\text{for m = 1...K do}:\\
&amp;\quad g_i=-\frac{\partial l(y_i, F_{m-1}(\mathbf{x}_i))}{\partial
F_{m-1}(\mathbf{x}_i)}\\
&amp;\quad \mathbf{w}_k = \underset{\mathbf{w}_k}{\text{arg min}}
\sum_{i=1}^n [g_i-\beta f(\mathbf{x}_k;\mathbf{w})]^2\\
&amp;\quad \rho_k = \underset{\rho}{\text{arg min}}\sum_{i=1}^nl(y_i,
F_{k-1}(\mathbf{x})+\rho f_k(\mathbf{x};\mathbf{w}_k)) \\
&amp;\quad F_k(\mathbf{x})=F_{k-1}(\mathbf{x})+\rho
f_k(\mathbf{x};\mathbf{w}_k)
\end{aligned}\\
\]</span></p>
<h3 id="gradient-boosting-desicion-tree-gbdt">Gradient Boosting Desicion
Tree (GBDT)</h3>
<p>  当 <span class="math inline">\(f = tree\)</span>
时，即弱学习器为树模型时，<span class="math inline">\(F_k(\mathbf{x}) =
F_{k-1}(\mathbf{x})+\rho_k\sum_{j=1}^Jb_{jk}\text{I}(x\in R_{jk}) =
F_{k-1}(\mathbf{x})+\sum_{j=1}^J\gamma_{jk}\text{I}(\mathbf{x}\in
R_{jk})\)</span>。 <span class="math display">\[
\begin{aligned}
    &amp;1.\ F_0(\mathbf{x}) = \underset{\gamma}{\text{arg min}}
\sum_{i=1}^n l(y_i, \gamma)\\
    &amp;2.\ \text{for m = 1...K do}: \\
    &amp;3.\ \quad g_{im}=-\frac{\partial l(y_i,
F_{m-1}(\mathbf{x}_i))}{\partial F_{m-1}(\mathbf{x}_i)}\\
    &amp;4.\ \quad 以 {g_{im}} 为目标去拟合一颗树 tree_i = \{R_{jm}\}\\
    &amp;5.\ \quad \gamma_{jm} = \underset{\gamma_{jm}}{\text{arg
min}}\sum_{\mathbf{x}_i\in R_{jm}}l(y_i,
F_{m-1}(\mathbf{x}_i)+\gamma_{jm})\quad (即第 m 颗树的第 j
个叶子节点的输出值)\\
    &amp;6.\ \quad F_m(\mathbf{x}) = F_{m-1}(\mathbf{x}) +
\sum_{j=1}^J\gamma_{jm}I(\mathbf{x}\in R_{jm})
\end{aligned}
\]</span> + 可以看到，GBDT 本身非常简单，拟合树的过程一般是 CART
算法，损失函数 <span class="math inline">\(l\)</span> 回归问题一般用
<span class="math inline">\(MSE = (y - \hat{y})^2/2\)</span>，二分类用
<span class="math inline">\(LogLoss =
log(1+e^{-y\hat{y}})\)</span>，多分类一般用 <span
class="math inline">\(LogSoftmax =
-\sum_{k=1}^Ky_klog(\hat{y}_k)\)</span> + 注意，第 <span
class="math inline">\(4\)</span> 步和第 <span
class="math inline">\(5\)</span> 步，相当于计算了两遍 <span
class="math inline">\(\gamma_{jm}\)</span></p>
<hr />
<h2 id="xgboost">XGBoost</h2>
<p>  正如论文标题 (A Scalable Tree Boosting System)
说描述，该论文提出了基于梯度提升的一套完整的系统方法论。理论只是一方面，还有很多工程上的设计点。</p>
<h3 id="理论">理论</h3>
<p>  按照论文中的表示发 <span class="math inline">\(F=\{f(\mathbf{x}) =
w_{q(\mathbf{x})}\},\ q(\mathbf{x}): R^m\rightarrow T,\ \mathbf{w}\in
R^T\)</span>，<span class="math inline">\(T\)</span>
是叶子节点的个数，<span class="math inline">\(\mathbf{w}\)</span>
是所有叶子节点集合，<span class="math inline">\(w_i\)</span> 代表了第
<span class="math inline">\(i\)</span> 个叶子结点上的值，<span
class="math inline">\(q\)</span> 即树的结构，<span
class="math inline">\(q_i\)</span> 即第 <span
class="math inline">\(i\)</span> 颗树的结构。每轮迭代生成一棵树 <span
class="math inline">\(q_t\)</span>。<br />
  假设我们已经迭代了 <span class="math inline">\(t-1\)</span>
轮，迭代至第 <span class="math inline">\(t-1\)</span> 轮时的模型
(<code>model</code>) 针对第 <span class="math inline">\(i\)</span>
个样本的预测值为 <span
class="math inline">\(\hat{y}_i^{(t-1)}\)</span>，那么第 <span
class="math inline">\(t\)</span> 轮迭代时的目标 <span
class="math inline">\(L\)</span> 为： <span class="math display">\[
    L^{(t)} = \sum_{i=1}^n l(y_i,
\hat{y}_i^{(t-1)}+f_t(\mathbf{x}_i))+\Omega(f_t)
\]</span> 在 <span class="math inline">\((y_i,
\hat{y}_i^{(t-1)})\)</span> 处做二阶泰勒展开有： <span
class="math display">\[
\begin{aligned}
    L^{(t)} &amp;= \sum_{i=1}^nl(y_i,
\hat{y}_i^{(t-1)})+g_if_t(\mathbf{x}_i)+\frac{1}{2}h_if_t^2(\mathbf{x}_i)
+ \Omega(f_t)\\
    &amp;=\sum_{i=1}^n[g_if_t(\mathbf{x}_i)+\frac{1}{2}h_if_t^2(\mathbf{x}_i)]
+ \Omega(f_t)\\
    &amp;=
\sum_{i=1}^n[g_if_t(\mathbf{x}_i)+\frac{1}{2}h_if_t^2(\mathbf{x}_i)] +
\gamma T + \frac{1}{2}\lambda \sum_{j=1}^Tw_j^2\\
    g_i &amp;= \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial
\hat{y}_i^{(t-1)}}\\
    h_i &amp;= \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial
\hat{y}_i^{(t-1)}}
\end{aligned}
\]</span> 定义 <span class="math inline">\(I_j = \{i|q(\mathbf{x}_i) =
j\}\)</span>，即属于第 <span class="math inline">\(j\)</span> 个叶子节点
(第 <span class="math inline">\(t\)</span> 轮) 的样本集合。
那么按照叶子节点 <span class="math inline">\(j\)</span> 归并损失函数，
<span class="math inline">\(L^{(t)}\)</span> 可以化为： <span
class="math display">\[
    L^{(t)} = \sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j +
\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\lambda T
\]</span> 这是一个简单的二次函数，极值点在： <span
class="math display">\[
    w_j^*=-\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda}
\]</span> 损失函数的极值为： <span class="math display">\[
    L^{(t)} = -\frac{1}{2}\sum_{j}^T\frac{(\sum_{i\in
I_j}g_i)^2}{\sum_{i\in I_j}h_i + \lambda} + \gamma T
\]</span> 针对某个分裂节点 <span
class="math inline">\(j\)</span>，算出分裂前后的损失函数的变动为： <span
class="math display">\[
    \Delta L_j = \frac{1}{2}[\frac{(\sum_{i\in I_L}g_i)^2}{\sum_{i\in
I_L}h_i + \lambda} + \frac{(\sum_{i\in I_R}g_i)^2}{\sum_{i\in I_R}h_i +
\lambda} - \frac{(\sum_{i\in I}g_i)^2}{\sum_{i\in I}h_i +
\lambda}]-\gamma
\]</span></p>
<h3 id="精确贪心算法">精确贪心算法</h3>
<p><img src = "/images/xgboost_exact_greedy_algorithm.png" width=480><br />
这个算法好像有点问题，<span class="math inline">\(I\)</span>
表示只是当前节点的样本集合，但是 <span class="math inline">\(k\)</span>
又是遍历所有
<code>Feature</code>，该过程似乎只是描述了一次分裂的过程，并没有给出整个树的构建过程。而且算法中的
<span class="math inline">\(gain\)</span> 初始化之后就再也没用了。</p>
<h3 id="近似算法-weighted-quantile-sketch">近似算法 (Weighted Quantile
Sketch)</h3>
<p>  近似算法的核心是依据各个<code>feature</code>的分位数进行采样，采样出
<span class="math inline">\(l\)</span> 个点，然后分裂算法只在这 <span
class="math inline">\(l\)</span>
个点上进行。那么<strong>如何计算分位数</strong>变成了核心所在，也是本文的<strong>亮点</strong>所在。假设全体样本的第
<span class="math inline">\(k\)</span> 维特征值及样本对应二阶偏导数的
<span class="math inline">\(h_i\)</span> 对应的集合为: <span
class="math inline">\(D_k = \{(x_{1k}, h_1), ..., (x_{nk},
h_n)\}\)</span>。给定分位数的抽取标准 (<code>rank function</code>):
<span class="math display">\[
    r_k(z) = \frac{\sum_{(x, h)\in D_k, x&lt;z}h}{\sum_{(x, h)\in D_k}h}
\]</span> 找出这样一系列的 <span class="math inline">\(l\)</span> 个点
<span class="math inline">\(\{s_{k1}, ...,
s_{kl}\}\)</span>，满足一下约束： <span class="math display">\[
    |r_k(s_{k,j}) - r_k(s_{k,j-1})|&lt;\epsilon,\
s_{k,1}=\underset{i}{min}\ x_{i,k},\ s_{k,l}=\underset{i}{max}\ x_{i,k}
\]</span> 一图胜千言，即类似下图这样的采样。
<img src="/images/xgboost_approximate_rank_function.jpg" width=480></p>
<p>  可以看到由于 <span class="math inline">\(r_k(z)\)</span>
是基于二阶导数 <span class="math inline">\(h\)</span>
计算的，该采样过程每个样本的权重为 <span
class="math inline">\(h_i\)</span>。如果相邻点的 <span
class="math inline">\(x\)</span> 累加的 <span
class="math inline">\(h\)</span>
变动小，那么该点附近的采样就会比较稀疏，反之则比较密集。至于如何采样，能够适应<strong>大规模数据</strong>，作者将算法
(Weighted Quantile Sketch) 放在了附录，此处不展开。<br />
  当我们知道<strong>找分位数</strong>的标准以及<strong>如何找分位数</strong>后，就可以上近似算法了，近似算法找到
<span class="math inline">\(l\)</span> 个分位点后，根据 <span
class="math inline">\(l\)</span>
个分位点进行分桶，对桶内的所有数据计算其 <span class="math inline">\(G,
H\)</span> 之和 (即把一个桶当做“一个点”来算)，然后对 <span
class="math inline">\(l\)</span> 个“点”重复
<code>Exact Greedy Algorithm</code>。</p>
<p>Approximate Algorithm for Split Finding
<img src = "/images/xgboost_approximate_algorithm.png" width=480></p>
<ul>
<li><p>一开始选好，然后每次树切分都不变，<span
class="math inline">\(min_i\ x_{i,k},\ max_i\
x_{i,k}\)</span>也就是说是在总体样本里选，这就是我们之前定义的
<code>global proposal</code></p></li>
<li><p>每次确定好切分点的分割后样本也需要进行分割， <span
class="math inline">\(min_i\ x_{i,k},\ max_i\ x_{i,k}\)</span>
来自子树的样本集 <span class="math inline">\(D_k\)</span> ，这就是
<code>local proposal</code></p></li>
</ul>
<h3 id="其他优化">其他优化</h3>
<ul>
<li><p>缺失值处理</p>
<ol type="1">
<li>缺损值，2. 很多0值，以及 3. one-hot 编码等都会造成 <span
class="math inline">\(\mathbf{x}\)</span> 的稀疏性。</li>
</ol>
<ul>
<li>训练过程：
<ul>
<li>对于第 <span class="math inline">\(k\)</span>
维的特征值，先找出全部非缺损值样本 <span
class="math inline">\(I_k\)</span> 下的最佳分裂点</li>
<li>然后将有缺损值的样本全部添加到左子树，计算出 <span
class="math inline">\(L_{split}\)</span>。重复该过程至右子树，计算出
<span class="math inline">\(L_{split}&#39;\)</span>。比较较大者，作为第
<span class="math inline">\(k\)</span> 维特征的默认缺损方向
(<code>default direction</code>)</li>
</ul></li>
<li>预测过程：
<ul>
<li>对于缺损值样本，如果训练过程有记录
<code>default direction</code>，则按照 <code>default direction</code>
划分预测样本。</li>
<li>若没有，则 <code>default direction</code> 默认为左侧。</li>
</ul></li>
<li>原算法过程如下：
<img src = "/images/xgboost_algorithm3.png" width=480></li>
</ul></li>
<li><p>并行优化</p>
<p>  算法中最耗时的过程就是按照各个列的值对 <span
class="math inline">\(\mathbf{x}\)</span>
进行排序。针对这个情况，作者提出了一个叫做 <code>Column Block</code>
的数据结构。论文中讲得感觉不是很清楚，但是根据其名字直译
<code>列块</code> 和 论文中的图示大概可以推断。这种结构应该是预排序
<span class="math inline">\(x\)</span> 的 <code>index</code>，然后该
<code>index</code> 作为指针指向一个样本，<span
class="math inline">\(m\)</span> 个列对应于 <span
class="math inline">\(m\)</span> 个 <code>Column Block</code><br />
  这种数据结构对 <code>Exact Greedy Algorithm</code> 和
<code>Approximate Algorithm</code> 都有用。近似算法的采样过程可以在所有
<code>Column</code> 上并行</p></li>
<li><p>缓存优化</p>
<p>  提出 <code>Column Block</code>
数据结构后，又有新的问题出现。算法中每次梯度迭代时会累加更新，而每个<code>Column Block</code>是根据特征值排序的，跟原始的样本的梯度存储顺序不同，内存的更新不是连续的。针对此，作者提出了
<code>Cache-Aware Prefetch Algorithm</code>，在每个线程内部开辟一个内存缓冲区
(<code>internal buffer</code>)，缓冲区上按照 <code>mini-batch</code>
的方式分批次一批一批地更新梯度。</p></li>
<li><p>缓存外计算</p>
<p>  将数据进行分块存在磁盘上，然后运行时并行地去拉去数据。但是这样还不够，因为读磁盘是高耗时行为。</p>
<ul>
<li><p>Block Compression</p>
<p>将分块数据按照<strong>列</strong>进行压缩，先加载进内存再进行解压，用减少的数据量来
trade off 解压时间。</p></li>
<li><p>Block Sharding</p>
<p>将数据存在多个可用磁盘上</p></li>
</ul></li>
<li><p>Dart技术</p>
<p>dart
技术论文里没有提，但是库里面有实现，主要是一种防止过拟合的技术。Ref: <a
target="_blank" rel="noopener" href="http://proceedings.mlr.press/v38/korlakaivinayak15.pdf">DART:
Dropouts meet Multiple Additive Regression Trees</a></p></li>
</ul>
<hr />
<h2 id="lightgbm">LightGBM</h2>
<p>   GBDT
是一整套演进的方法论，随着数据量的爆炸式增长，对基础的算法原理的实现方式提出更大的挑战。XGBoost是一套完整的实现方案
(<code>Implementation</code>)。LightGBM
更是进一步的集大成者，由微软研究并开源其库，其基础的改进主要在提升训练速度上，当然库里面的改进点不仅仅只有论文里的原理。这里先整理一下论文里的原理，再逐步剖析
<code>LightGBM</code> 库里的亮点 <code>Feature</code>。<br />
   LightGBM论文新提出的思想很简单，都是针对训练优化加速的。分别是
<code>Gradient Based One-Side Sampling (GOSS)</code> 和
<code>Exclusive Feature Bundling (EFB)</code>。LightGBM
还顺便提了一下直方图算法 <code>Histogram Algorithm</code>，其实就是
XGBoost 里面的近似算法。<br />
  
除了论文里所说的加速优化算法之外，<code>LightGBM</code>还有很多别的优化点论文里没提。这个在其官方文档里面有说明，这里也一并归纳一下。</p>
<h3 id="goss">GOSS</h3>
<p>   <code>GOSS</code>的想法也不复杂，每次迭代时选出比率为 <span
class="math inline">\(a\)</span>
的<strong>大梯度的样本</strong>，然后再随机选出 <span
class="math inline">\(b\)</span>
比率的小梯度的样本作为<strong>随机样本</strong>。为了保证精度一致性，对<strong>随机样本</strong>进行一个
<span class="math inline">\(\frac{1-a}{b}\)</span>
地加权。用这部分样本进行训练，从而大大减少训练样本的数量进而进行加速。算法如下：
<img src = "/images/lightGBM_GOSS.png" width=480><br />
论文从理论上证明了这样采样的后的训练精度渐进趋近与使用全样本进行训练的训练精度差。</p>
<h3 id="efb">EFB</h3>
<p>  
<code>EFB</code>分为两个阶段，阶段一：将“几乎”互斥的特征捆绑在一起形成一个
<code>bundle</code>；阶段二：将同一个<code>bundle</code>内的特征值进行合并使之成为一个真正可用的
"<code>feature</code>"。</p>
<ul>
<li><p>阶段一：Greedy Bundling</p>
<p><img src = "/images/lightGBM_EFB1.png" width=480></p>
<p>别看算法截图挺唬人，论文里已经非常详细地说清楚了算法的实现过程。这边直译一下：“第一步：构图，图的节点是每个
<code>Feature Column</code>，边是带权重边，其权重为两个 Feature
的冲突数量。第二步：将节点按照其度 (<code>degree</code>)
进行降序排序；第三步：依次检查每个
<code>Feature</code>，计算跟已经存在的 <code>bundle</code>
的冲突数量，如果小于一个阈值 <span
class="math inline">\(\gamma\)</span>，那么就将该 <code>Feature</code>
加入这个 <code>bundle</code>。反之，则添加一个新的
<code>bundle</code></p></li>
<li><p>阶段二：Merge Exclusive Features</p>
<p><img src = "/images/lightGBM_EFB2.png" width=480></p>
<p>算法看着也挺唬人，论文里面举了个很好的例子来说明该算法，可以自己手动举例来感受一下。这里直接贴原话：“Originally,
feature A takes value from [0; 10) and feature B takes value [0; 20). We
then add an offset of 10 to the values of feature B so that the refined
feature takes values from [10; 30). After that, it is safe to merge
features A and B, and use a feature bundle with range [0; 30] to replace
the original features A and B.”</p></li>
</ul>
<h3 id="leaf-wise-learning">Leaf-wise Learning</h3>
<p>  基于叶子节点的生长策略。许多决策树的生长策略是 level(depth)-wise
的，每次分裂同等地对待每个节点。而 LightGBM 是 leaf-wise
的，每次分裂选择 loss 降低最大的叶子节点进行分裂。原理详见 <a
target="_blank" rel="noopener" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.2862&amp;rep=rep1&amp;type=pdf">Best-First
Decision Learning</a></p>
<ul>
<li>level-wise
<img src = "/images/lightGBM_level-wise.png" width = 480></li>
<li>leaf-wise
<img src = "/images/lightGBM_leaf-wise.png" width = 480></li>
</ul>
<h3 id="histogram-substraction">Histogram Substraction</h3>
<p>  在计算直方图时，一个节点的直方图可以由其父节点的直方图减去邻居节点的直方图算得。所以每次只需要计算一个具有少量数据的节点的直方图，其邻居节点的直方图可以直接得出。从而减少计算量。</p>
<h3 id="feature-parallel">Feature Parallel</h3>
<p>  并行算法主要是适应大数据情况下的<strong>分布式训练任务</strong>。 +
传统特征并行： 1. 垂直(按列)分割数据
(每个机器有不同的特征子集和全量样本) 2. 每个 <code>Worker</code>
基于本地的分割数据找到本地的最佳分割点 (<code>best split point</code>)
3. 所有 <code>Worker</code> 交换最佳分裂点，得到全局最佳分裂点。 4.
有全局最佳分裂点的 <code>Worker</code> 进行分裂，并将分裂结果分发至其他
<code>Worker</code> (这一步网络最耗时，因为涉及到样本的分发) 5. 其他
<code>Worker</code> 基于接收到的结果进行再次分裂</p>
<ul>
<li><p>LightGBM特征并行：</p>
<p>为了解决上诉第 4 步的高网络耗时问题，LightGBM 让每个
<code>Worker</code> 持有全量数据。这样网络开销就只有第 3 步，即所有
<code>Worker</code> 交换最佳分裂节点的信息。然后分裂就在本地执行了。</p>
<ol type="1">
<li>每个 <code>Worker</code> 基于本地数据找到最佳分裂点。</li>
<li>所有 <code>Worker</code> 交换最佳分裂点，得到全局最佳分裂点。</li>
<li>进行全局最佳分裂</li>
</ol></li>
</ul>
<h3 id="data-parallel">Data Parallel</h3>
<ul>
<li><p>传统数据并行：</p>
<ol type="1">
<li>水平(按行)分割数据 (每个机器有全部的特征和部分样本)</li>
<li>每个 <code>Worker</code> 基于本地数据建立直方图</li>
<li>所有 <code>Worker</code> 交换直方图信息，得到全局直方图
(最耗时)</li>
<li>基于全局直方图得到最佳分裂点</li>
</ol>
<p>传统的数据并行，如果采用 <code>point-to-point</code>
算法，则时间复杂度为 <span class="math inline">\(O(n_{machine}\cdot
n_{feature} \cdot n_{bin})\)</span>。如果采用 <code>All-Reduce</code>
算法，则时间复杂度为 <span class="math inline">\(O(2\cdot n_{feature}
\cdot n_{bin})\)</span></p></li>
<li><p>LightGBM数据并行：</p>
<ul>
<li>针对上诉第 3 步，LightGBM 采用了一个名为 <code>Reduce Scatter</code>
的算子来从不同的 <code>Worker</code> 上合并不同 <code>Feature</code>
的直方图。至于这个算子到底是啥，就只能看源码了</li>
<li>如前言的 <code>Histogram Substraction</code>
算法，交换直方图时可以只交换一个叶子节点的直方图，其邻居节点的直方图可以直接减法得到。</li>
</ul></li>
</ul>
<h3 id="voting-parallel">Voting Parallel</h3>
<hr />
<h1 id="gbdt-在-ranking-领域的应用">GBDT 在 Ranking 领域的应用</h1>
<h2 id="ranknet">RankNet</h2>
<p>   对应 Rank 问题，我们先约定相关的符号变量定义。对于某两个文档 <span
class="math inline">\(U_i\)</span> 和 <span
class="math inline">\(U_j\)</span>，其对应的由模型预测的分数分别为 <span
class="math inline">\(s_i = f(\mathbf{x}_i; \mathbf{w})\)</span> 和
<span class="math inline">\(s_j = f(\mathbf{x}_j;
\mathbf{w})。\)</span>如果 <span class="math inline">\(U_i\)</span>
排在了 <span class="math inline">\(U_j\)</span> 前面，则用关系 <span
class="math inline">\(U_i &gt; U_j\)</span> 表示，如果文档 <span
class="math inline">\(U_i\)</span> 和 <span
class="math inline">\(U_j\)</span> 的价值相等，则表示 <span
class="math inline">\(U_i = U_j\)</span>。用概率 <span
class="math inline">\(P_{ij}\)</span> 表示文档 <strong>应该</strong>
<span class="math inline">\(U_i\)</span> 被排在了 <span
class="math inline">\(U_j\)</span> 的前面的概率 (真值)，<span
class="math inline">\(\hat{P}_{ij}\)</span>
表示文档<strong>模型预测</strong>出来的 <span
class="math inline">\(U_i\)</span> 被排在了 <span
class="math inline">\(U_j\)</span> 前面的概率。那么我们可以用
<code>logistic</code> 来计算 <span
class="math inline">\(\hat{P}_{ij}\)</span>，即： <span
class="math display">\[
  \hat{P}_{ij} = \frac{1}{1+e^{-\sigma(s_i - s_j)}}
\]</span></p>
<ul>
<li>如果 <span class="math inline">\(U_i &gt; U_j\)</span>，则模型预测的
<span class="math inline">\(s_i - s_j \rightarrow +\infty\)</span> ，则
<span class="math inline">\(\hat{P}_{ij}\rightarrow 1\)</span>，即<span
class="math inline">\(U_i\)</span> <strong>越有可能</strong>排在 <span
class="math inline">\(U_j\)</span> 的前面。如果 <span
class="math inline">\(U_i &lt; U_j\)</span>，则模型预测的 <span
class="math inline">\(s_i - s_j\rightarrow -\infty\)</span>，则 <span
class="math inline">\(\hat{P}_{ij} \rightarrow 0\)</span>， 即 <span
class="math inline">\(U_i\)</span> <strong>越不可能</strong> 排在 <span
class="math inline">\(U_j\)</span> 的前面。</li>
</ul>
<p>用 <code>CrossEntropy</code> 来计算预测和真实之间的损失</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;C_{ij} = -P_{ij}log(\hat{P_{ij}}) - (1-P_{ij})log(\hat{P_{ij}})\\
  &amp;P_{ij} = \frac{1}{2}(1+S_{ij}) \\
  &amp;S_{ij} = \begin{cases}
    0, &amp; U_i = U_j\\
    1, &amp; U_i &gt; U_j \\
    -1, &amp; U_i &lt; Uj
  \end{cases}
  \end{aligned}
\]</span></p>
<p>带入 <span class="math inline">\(P_{ij},\ \hat{P}_{ij},\
S_{ij}\)</span>，我们有：</p>
$$
<span class="math display">\[\begin{aligned}
  C_{ij} &amp;= \frac{1-S_{ij}}{2}\sigma(s_i - s_j) +
log[1+e^{-\sigma(s_i - s_j)}]\\

  \frac{\partial C_{ij}}{\partial s_i} &amp;= \sigma[\frac{1-S_{ij}}{2}
- \frac{1}{1+e^{\sigma(s_i - s_j)}}]  = -\frac{\partial C_{ij}}{\partial
s_j}
\end{aligned}\]</span>
<p>$$</p>
<p>令：<span class="math inline">\(\lambda_{ij} =
\sigma[\frac{1-S_{ij}}{2} - \frac{1}{1+e^{\sigma(s_i -
s_j)}}]\)</span>，则 <span class="math inline">\(\frac{\partial
C_{ij}}{\partial s_i} = \lambda_{ij}\)</span></p>
<ul>
<li>注意这里的 <span class="math inline">\(\lambda_{ij}\)</span> 是针对
<span class="math inline">\(\{i,j\}\)</span> pair 对 <span
class="math inline">\(i\)</span> 的偏导。比如 <span
class="math inline">\(\lambda_{12}\)</span> 是 <span
class="math inline">\(\{1,2\}\)</span> 组合中对文档 <span
class="math inline">\(1\)</span> 的偏导。<span
class="math inline">\(\lambda_{23}\)</span> 是 <span
class="math inline">\(\{2,3\}\)</span> 组合中对文档 <span
class="math inline">\(2\)</span> 的偏导，对文档 <span
class="math inline">\(3\)</span> 的偏导为 <span
class="math inline">\(-\lambda_{23}\)</span></li>
</ul>
<p>   对于 <span class="math inline">\(\{i, j\}\)</span>
的组合，我们不妨约定 <span class="math inline">\(U_i &gt; U_j\)</span>
(论文里没有讨论 <span class="math inline">\(U_i = U_j\)</span>
的情况，记为 <span
class="math inline">\((i,j)\)</span>。这样可以大大化简公式。为什么这样约定是可行的，比如针对
<span class="math inline">\(U_1, U_3\)</span> 文档组合 <span
class="math inline">\(\{1, 3\}\)</span> 和 <span
class="math inline">\(\{3,1\}\)</span> 表示的是同一对组合。假设 <span
class="math inline">\(U_3 &gt; U_1\)</span>，则我们采用 <span
class="math inline">\((3, 1)\)</span> 的 pair 记录方式。
约定之后，我们有：</p>
<p><span class="math display">\[
  \lambda_{ij} = -\frac{\sigma}{1+e^{\sigma(s_i - s_j)}} \\
  C_{ij} = log[1+e^{-(s_i - s_j)}]
\]</span></p>
<ul>
<li>比如针对 4 个 文档，排序后的顺序为 <span
class="math inline">\((1,3,2,4)\)</span>，那么我们有 6 个 pair 对 <span
class="math inline">\((1,2), (1,3), (1,4), (3,2), (2,4),
(3,4)\)</span></li>
<li>我们来看负梯度方向 <span class="math inline">\((-\lambda_{ij},
\lambda_{ij})\)</span> 是 <span class="math inline">\(C_{ij}\)</span>
下降最快的方向，<span class="math inline">\(-\lambda_{ij} =
\frac{\sigma}{1+e^{\sigma(s_i - s_j)}}\)</span>
是恒大于0的，类似于一种“力”，把 <span
class="math inline">\(s_i\leftarrow s_i+\lambda_{ij}\)</span>
往前(上)抬，把 <span class="math inline">\(s_j\)</span> 往下压。</li>
<li>如果 <span class="math inline">\(s_i - s_j \gg 0\)</span>，则有
<span class="math inline">\(-\lambda_{ij} \rightarrow 0\)</span>，即
<span class="math inline">\(s_i\)</span> 方向上基本上不需要更新了。如果
<span class="math inline">\(s_i - s_j \ll 0\)</span>，则有 <span
class="math inline">\(-\lambda_{ij} \rightarrow +\infty\)</span>，即
<span class="math inline">\(s_i\)</span> 方向上会有很大的步长更新。</li>
</ul>
<p><span class="math inline">\(C_{ij}\)</span> 对 <span
class="math inline">\(\mathbf{w}\)</span> 的偏导为：</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial C_{ij}}{\partial \mathbf{w}}
&amp; = \frac{\partial C_{ij}}{\partial s_i}\frac{\partial s_i}{\partial
\mathbf{w}}+\frac{\partial C_{ij}}{\partial s_j}\frac{\partial
s_j}{\partial \mathbf{w}}\\
&amp; = \lambda_{ij}(\frac{\partial s_i}{\partial \mathbf{w}} -
\frac{\partial s_j}{\partial \mathbf{w}})\\
\end{aligned} \\
\]</span></p>
<p>对于 <span class="math inline">\(N\)</span> 个文档，总共有 <span
class="math inline">\(\frac{(N-1)(N-2)}{2}\)</span> 个 pair 对。对于
<span class="math inline">\(N\times N\)</span> 的组合矩阵，这些 pair
对即其除去对角线以外的上(下)三角 pair元素。假设这些 pair
对组成的全体集合为：<span class="math inline">\(I = \{\{i,j\}\}\)</span>
(用大括号而不是小括号说明 <span class="math inline">\(\{U_i,
U_j\}\)</span> 和 <span class="math inline">\(\{U_j, U_i\}\)</span>
是一个元素。则全体成员上的损失为： <span class="math display">\[
  C = \sum_{\{i,j\}\in I}C_{ij} \\
\]</span> 则有基于 N 个 doc 的梯度下降法为： <span
class="math display">\[
\begin{aligned} \\
  &amp; \mathbf{w}\leftarrow \mathbf{w}-\eta\frac{\partial C}{\partial
\mathbf{w}} \\
  &amp; \mathbf{w}\leftarrow \mathbf{w}-\eta \sum_{\{i,j\}\in
I}\frac{\partial C_{ij}}{\partial \mathbf{w}}\\
  &amp; \mathbf{w}\leftarrow \mathbf{w}-\eta \sum_{\{i,j\}\in
I}\frac{\partial C_{ij}}{\partial s_i}\frac{\partial s_i}{\partial
\mathbf{w}}+\frac{\partial C_{ij}}{\partial s_j}\frac{\partial
s_j}{\partial \mathbf{w}} \\
\\
\end{aligned}
\]</span></p>
<p>梯度下降法每次更新权重时，要在全体的 pair 对上做前向 (算<span
class="math inline">\(\lambda_{ij})\)</span> 和 后向 (算偏导，更耗时)
运算，其整体的时间复杂度(一个query下)为 <span
class="math inline">\(O(N^2)\)</span></p>
<h3 id="梯度下降法加速">梯度下降法加速</h3>
<p>  看前向后向的计算过程，后向过程 <span
class="math inline">\(\frac{\partial s_i}{\partial \mathbf{w}}\)</span>
其实只需对全体 <span class="math inline">\(N\)</span>
个文档计算一次，<span class="math inline">\(\partial s_i 和 \partial
s_j\)</span> 可以重复利用。只有前向过程 <span
class="math inline">\(\frac{\partial C_{ij}}{\partial s_i}\)</span> 和
作用在所有的 pair 对上。上诉一个 pair 对 <span
class="math inline">\(C_{ij}\)</span> 会拆出两项出来。我们对 <span
class="math inline">\(\sum_{\{i,j\}\in I}\)</span> 按 <span
class="math inline">\(i\)</span> 拆解，对于所有包含 <span
class="math inline">\(i\)</span> 的 pair 对有：</p>
<p><span class="math display">\[
\begin{aligned}
  &amp; \sum_{\{i,j\}\in I}\frac{\partial C_{ij}}{\partial
s_i}\frac{\partial s_i}{\partial \mathbf{w}}+\frac{\partial
C_{ij}}{\partial s_j}\frac{\partial s_j}{\partial \mathbf{w}}\\
  &amp; = \sum_{i\in N}\frac{\partial s_i}{\partial
\mathbf{w}}(\sum_{(i,j)\in I}\frac{\partial C_{ij}}{\partial s_i}
+\sum_{(j,i)\in I}\frac{\partial C_{ij}}{\partial s_j})\\
  &amp; = \sum_{i\in N}\frac{\partial s_i}{\partial
\mathbf{w}}(\sum_{(i,j)\in I}\lambda_{ij} - \sum_{(j,i)\in
I}\lambda_{ij})\\
  &amp; = \sum_{i\in N}\lambda_i\frac{\partial s_i}{\partial \mathbf{w}}
\end{aligned}
\]</span></p>
<p>则有： <span class="math inline">\(\mathbf{w}\leftarrow
\mathbf{w}-\eta \sum_{i}\lambda_i\frac{\partial s_i}{\partial
\mathbf{w}}\)</span>，其中 <span class="math inline">\(\lambda_{i} =
\sum_{(i,j)\in I}\lambda_{ij}-\sum_{(j,i)\in
I}\lambda_{ij}\)</span>。</p>
<ul>
<li>对比 RankNet 和 XGBoost 对损失函数的化简过程，发现理清
<strong>整个样本集和损失函数之间的对应关系</strong>
是化简过程的关键所在</li>
<li>关于加速过程，可以具体举 <span class="math inline">\(N\)</span> 个
doc 例子看一下。</li>
</ul>
<p>仔细观察 <span class="math inline">\(\lambda_{ij}\)</span> 和 <span
class="math inline">\(\lambda_{i}\)</span>。梯度下降法的过程是这样的：</p>
<ol type="1">
<li><span class="math inline">\(\mathbf{w}\)</span> 随机初始化</li>
<li>计算出模型 <span class="math inline">\(f\)</span> 对于所有文档
的预测值 <span class="math inline">\(s_i\)</span>，以及对当前 <span
class="math inline">\(\mathbf{w}\)</span> 的梯度。</li>
<li>对于包含文档 <span class="math inline">\(i\)</span> 的 所有 pair 对
<span class="math inline">\(\{i, j\}\)</span> 以及 <span
class="math inline">\(\{j, i\}\)</span>，计算 <span
class="math inline">\(\lambda_i\)</span>（比如针对 <span
class="math inline">\(4\)</span> 个文档集合，包含文档 <span
class="math inline">\(2\)</span> 的 pair 对为 <span
class="math inline">\(\{1,2\}, \{2,3\}, \{2,4\}\)</span>）</li>
<li>计算 <span class="math inline">\(\delta \mathbf{w} = -\eta
\sum_{i}\lambda_i\frac{\partial s_i}{\partial \mathbf{w}}\)</span></li>
<li>更新 <span class="math inline">\(\mathbf{w}\leftarrow
\mathbf{w}+\delta\mathbf{w}\)</span></li>
</ol>
<h2 id="ir-评价指标">IR 评价指标</h2>
<ul>
<li><strong>NDCG (Normalized Discounted Cumulative Gain)</strong><br />
NDCG at position n： 假设：
<ul>
<li><p>有 <span class="math inline">\(n\)</span> 个 <code>doc</code>:
<span class="math inline">\(\{U_1,
...,U_n\}\)</span>，每个<code>doc</code>的<strong>标注相关性</strong>为
<span
class="math inline">\(l_{U_i}\)</span>，<strong>预测相关性</strong>为
<span class="math inline">\(s_{U_i}\)</span>。</p></li>
<li><p>有 n 个位置，分别记为 <span
class="math inline">\([1,2,...,n]\)</span></p></li>
<li><p><span class="math inline">\(\pi\)</span> 为一种排列方式，在 <span
class="math inline">\(\pi: U^{(\pi)}_j = \pi(i)\)</span> 的排列方式 (即
<span class="math inline">\(i\)</span> 位置对应的 <code>doc</code> 为
<span class="math inline">\(U_j\)</span>)
下，<code>doc</code>的排序结果为：<span class="math inline">\([\pi(1),
..., \pi(n)]\)</span>，对应的分数排序为 <span
class="math inline">\([s_{\pi(1)}, ..., s_{\pi(n)}]，\)</span><span
class="math inline">\(\pi^*\)</span>
为最佳排列方式。可见，最佳排序方式为
<code>[U_1, ...,U_n].sort(key = lambda U: l(U)</code>，即按照标签 <span
class="math inline">\(l\)</span> 去排序。则有： <span
class="math display">\[
  DCG@k(\pi) = \sum_{j=1}^k \frac{G(j;r)}{D(j)}\\
  G(j;r) = 2^{s_{\pi(j)} - 1},\ D(j) = {log(j+1)}\\
  NDCG@k(\pi) = \frac{DCG@k(\pi)}{DCG@k(\pi^*)}
\]</span> 其中 <span class="math inline">\(j\)</span> 表示
<code>Position</code>，<span class="math inline">\(G\)</span> 表示
<code>Gain</code>，<span class="math inline">\(D\)</span> 表示
<code>Position Discount</code>，则 <span
class="math inline">\(\sum\)</span> 表示
<code>Cumulating</code>。</p></li>
<li><p>一般的实现为<code>ndcg = dcg(y_pred, y_true)/dcg(y_true, y_true)</code>，其中
<code>y_pred</code> 为模型预测值，<code>y_true</code>
为真实的<code>label</code>。在<code>dcg()</code>里，将<code>y_true</code>
按照 <code>y_pred</code> 进行排序。</p>
<ul>
<li>衍生出的问题是：如果有两个<code>y_pred</code>
<strong>一样</strong>该怎么办呢？一样意味着相同分数下的 <code>doc</code>
可以有多种排序情况。
<ul>
<li>计算相同 <code>y_pred</code> 下的 <span
class="math inline">\(G\)</span> 均值和 <span
class="math inline">\(D\)</span> 均值，再求和。</li>
</ul></li>
<li>具体的实现可以参看 <code>sklearn.metrics.ndcg_score</code> 源码。
<ul>
<li>注意<code>sklearn</code> 源码中有一个 <code>ignore_ties</code>
选项，指的是如果 <code>y_pred</code>
存在相同的值，那么这些相同的值被视为一个 <code>tie</code>。此时在计算
<code>dcg</code>的时候会有一些区别，具体可见源码。</li>
<li><code>sklearn</code>当中的<code>Gain</code>的映射直接放在<code>y_true</code>里面。</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>MAP (Mean Average Precision)</strong> <span
class="math display">\[
      \begin{aligned}
          &amp;P@n=\frac{NumRelDoc(n)}{n}\\
          &amp;AP@n=\frac{\sum_k P@k \cdot
IsRelevant(doc_k)}{NumRelDoc(n)}\\
          &amp;MAP=\frac{\sum_{q=1}^QAP_q}{Q}
      \end{aligned}
  \]</span>
<ul>
<li>其中 <span class="math inline">\(NumRelDoc(n)\)</span> 表示前 <span
class="math inline">\(n\)</span> 位相关 doc 的数量。<span
class="math inline">\(IsRelevant(doc)\)</span> 表示该 <span
class="math inline">\(doc\)</span> 是否相关，为 0-1 函数。</li>
<li><span class="math inline">\(AP_q\)</span> 表示针对 <span
class="math inline">\(query = q\)</span> 的平均准确率</li>
</ul></li>
</ul>
<h2 id="lambdarank">LambdaRank</h2>
<p>   回看 RankNet 中损失函数 <span
class="math inline">\(C_{ij}\)</span> 对 <span
class="math inline">\(s_i\)</span> 的偏导数 <span
class="math display">\[
  \frac{\partial C_{ij}}{\partial s_i} = \lambda_{ij} =
-\frac{\sigma}{1+e^{\sigma(s_i - s_j)}}
\]</span> 这里的位置信息并没有被建模进去。<code>DCG</code>
指标中，越靠前的位置重要度越高。假设一批文档中只有两个相关的文档 <span
class="math inline">\(U_1\)</span> 和 <span
class="math inline">\(U_2\)</span>。假设 <span
class="math inline">\(U_1\)</span> 排在第 3 位，<span
class="math inline">\(U_2\)</span> 排在非常靠后的位置，那么模型更应该
<strong>花费一点点力气</strong> 去把 <span
class="math inline">\(U_1\)</span> 往前挪 1-2 位，而不是
<strong>花费巨大的力气</strong> 把 <span
class="math inline">\(U_2\)</span> 往前排很多位。</p>
<p>   排序问题的建模中，通常我们根据已经排好 <strong>后</strong>
的结果去规定文档的挪动规则，要比直接建立一个全局的，光滑可导的 <span
class="math inline">\(NDCG\)</span> 的近似函数要方便得多。 LambdaRank
的方法很直观，对于 <span class="math inline">\(\lambda_{ij}\)</span>
乘上一个 <strong>因子</strong>，用来衡量 <span
class="math inline">\(s_i\)</span> 方向上的
<strong>迭代强度</strong>，对于 <span
class="math inline">\(NDCG\)</span> 指标，该因子为 <span
class="math inline">\(|\Delta NDCG|\)</span>，表示将文档 <span
class="math inline">\(i\)</span> 和 <span
class="math inline">\(j\)</span> 交换后 <span
class="math inline">\(NDCG\)</span> 的变动幅度。对于其他指标，诸如 <span
class="math inline">\(MAP, ERR\)</span> 等指标，也可以写出相应的
<strong>因子</strong>。</p>
<h3 id="delta-ndcg-计算方法"><span class="math inline">\(\Delta
NDCG\)</span> 计算方法</h3>
<p>   因为 <span class="math inline">\(NDCG\)</span>
的分母为常数，我们只看交换文档 <span class="math inline">\(i,\
j\)</span> 后， <span class="math inline">\(DCG\)</span>
的变动。假设最佳排序 <span class="math inline">\(\pi^*\)</span>
方式下，<span class="math inline">\(i,\ j\)</span> 的排序位置分别是
<span class="math inline">\(r_i, r_j\)</span>。在 <span
class="math inline">\(\pi^*\)</span>的基础上，仅仅交换 <span
class="math inline">\(i, j\)</span> 俩文档位置对应的排序为 <span
class="math inline">\(\pi&#39;\)</span> <span class="math display">\[
\begin{aligned}
  DCG@k(\pi^*) &amp;= \frac{2^{l_i - 1}}{log(1+r_i)} + \frac{2^{l_j} -
1}{log(1+r_j)} + \sum_{r = 1, \pi^*(r) \neq
\{i,j\}}^k\frac{2^{s_{\pi^*(r)} - 1}}{log(1+r)} \\
  DCG@k(\pi&#39;) &amp;= \frac{2^{l_i - 1}}{log(1+r_j)} + \frac{2^{l_j
-1}}{log(1+r_i)} + \sum_{r = 1, \pi&#39; \neq
\{i,j\}}^k\frac{2^{s_{\pi&#39;} - 1}}{log(1+r)}\\
  |\Delta DCG| &amp;= |DCG@k(\pi^*) - DCG@k(\pi&#39;)|\\
               &amp;= |(2^{l_i - 1} - 2^{l_j - 1})(\frac{1}{log(1+r_i)}
- \frac{1}{log(1+r_j)})|
\end{aligned}
\]</span> 论文中对 <span class="math inline">\(|\Delta DCG|\)</span>
还乘了一个 <span class="math inline">\(N = max(DCG)\)</span></p>
<h3 id="对-lambda_ij-的改进">对 <span
class="math inline">\(\lambda_{ij}\)</span> 的改进</h3>
<p><span class="math display">\[
  \lambda_{ij} = -\frac{\sigma N}{1+e^{\sigma(s_i - s_j)}}|(2^{l_i - 1}
- 2^{l_j - 1})(\frac{1}{log(1+r_i)} - \frac{1}{log(1+r_j)})|\\
  \lambda_{ij} = -\frac{\sigma }{1+e^{\sigma(s_i - s_j)}}[N(2^{l_i - 1}
- 2^{l_j - 1})(\frac{1}{log(1+r_i)} - \frac{1}{log(1+r_j)})]
\]</span></p>
<ul>
<li>由于 <span class="math inline">\(U_i &gt; U_j\)</span>，实际上有
<span class="math inline">\(l_i &gt; l_j,\ r_i &lt;
r_j\)</span>，即绝对值符号可以去掉，原论文里面没有绝对值符号，但是后续相关的论文引用里面都有绝对值符号。感觉好些论文引用都搞得模棱两可的。</li>
<li>对比前面的 <span class="math inline">\(\lambda_{ij}\)</span> “力”
的解读，这里可以看到，越靠前的文档，给得“力”越大 (<span
class="math inline">\(\Delta DCG\)</span>) 因子的作用。</li>
</ul>
<h3 id="其他">其他</h3>
<p>   论文对如何引出 LambdaRank 思想给出了比较好的说明；以及 <span
class="math inline">\(\lambda\)</span> 函数的的存在性理论证明
(涉及到微分几何的知识)。太高深了看不懂，暂且不谈。</p>
<h2 id="mart">MART</h2>
<h3 id="二分类-pm-1-问题下的-gbdt-形式">二分类 <span
class="math inline">\(\pm 1\)</span> 问题下的 GBDT 形式</h3>
<p>   所谓 MART 就是前面讲得 GBDT 算法。当损失 <span
class="math inline">\(l\)</span> 为交叉熵损失函数时。约定 <span
class="math inline">\(y = \{\pm1\}\)</span>，则 <span
class="math inline">\(L(y, F_{m-1}(\mathbf{x})) =
\sum_{i}log(1+e^{-y_iF_{m-1}(\mathbf{x})})\)</span> (至于为什么 Loss
是这个形式，可以参考 <strong>Loss</strong> 章节，对这种 Loss
有比较深入的解读)。则有，对第 <span
class="math inline">\(F_{m-1}\)</span> 的偏导如下： <span
class="math display">\[
  \frac{\partial L}{\partial F_{m-1}} =
\sum_{i}\frac{-y_i}{1+e^{y_iF_{m-1}(\mathbf{x}_i)}} = \sum_{i}-y_iu_i
\]</span> 假设已经拟合好了第 <span class="math inline">\(m\)</span>
棵树，生成了 <span class="math inline">\(R_{jm}\)</span>
个子区域，则每个子区域的预测值 <span
class="math inline">\(\gamma_{jm}\)</span> 为： <span
class="math display">\[
  \begin{aligned}
  \gamma_{jm} &amp;= \underset{\gamma_{jm}}{\text{arg
min}}\sum_{\mathbf{x}_i\in R_{jm}}l(y_i,
F_{m-1}(\mathbf{x}_i)+\gamma_{jm}) \\
  &amp;= \underset{\gamma_{jm}}{\text{arg min}}\sum_{\mathbf{x}_i\in
R_{jm}}log[1+e^{-y_i[F_{m-1}(\mathbf{x}_i) + \gamma_{jm}]}]
  \end{aligned}
\]</span> <span class="math inline">\(\gamma_{jm}\)</span> 是 <span
class="math inline">\(\sum_{i}l\)</span> 的极值点，牛顿法: <span
class="math inline">\(x_{m} = x_{m-1} -
\frac{f&#39;(x)}{f^{&#39;&#39;}(x)}\)</span>
是数值求解极值点一种迭代算法。这里我们从 <span
class="math inline">\(\gamma^{(0)} = 0\)</span>
开始，只进行<strong>一步迭代</strong>来近似 <span
class="math inline">\(\gamma_{jm}\)</span> <span class="math display">\[
  \gamma_{jm} = -\frac{g&#39;(\gamma^{(0)})}{g&#39;&#39;(\gamma^{(0)})}
\\
  g(\gamma_{jm}) = \sum_{\mathbf{x}_i\in
R_{jm}}log[1+e^{-y_i[F_{m-1}(\mathbf{x}_i) + \gamma_{jm}]}]
\]</span> 可以算得： <span class="math display">\[
  g&#39;(\gamma_{jm}) = \sum_{\mathbf{x}_i\in
R_{jm}}\frac{-y_i}{1+e^{y_i[F_{m-1}(\mathbf{x})+\gamma_{jm}]}} \\
  g&#39;&#39;(\gamma_{jm}) = \sum_{\mathbf{x}_i\in
R_{jm}}\frac{y_i^2e^{y_i(F_{m-1}+\gamma_{jm})}}{[1+e^{y_i(F_{m-1}
+\gamma_{jm})}]^2}
\]</span> 带入 <span class="math inline">\(\gamma_{jm} = \gamma^{(0)} =
0\)</span> 有： <span class="math display">\[
  g&#39;(0) = \sum_{\mathbf{x}_i\in
R_{jm}}\frac{-y_i}{1+e^{y_iF_{m-1}(\mathbf{x})}} = \sum_{\mathbf{x}_i\in
R_{jm}} -y_iu_i \\
  g&#39;&#39;(0) = \sum_{\mathbf{x}_i\in
R_{jm}}\frac{y_i}{1+e^{y_iF_{m-1}}}\cdot\frac{y_ie^{y_iF_{m-1}}}{1+e^{y_iF_{m-1}}}=\sum_{\mathbf{x}_i\in
R_{jm}}u_i(1-u_i)
\]</span> 得到 <span class="math inline">\(\gamma_{jm}\)</span>
的近似值： <span class="math display">\[
  \gamma_{jm} = \frac{g&#39;(0)}{g&#39;&#39;(0)} =
-\frac{\sum_{\mathbf{x}_i\in R_{jm}} y_iu_i}{\sum_{\mathbf{x}_i\in
R_{jm}}u_i(1-u_i)}
\]</span></p>
<h3 id="算法">算法</h3>
<ol type="1">
<li>初始化 <span class="math inline">\(F_0(\mathbf{x}) =
\underset{\gamma}{\argmin} \sum_{i}l(y_i, \gamma)\)</span>，即对所有的
<span class="math inline">\(\mathbf{x}\)</span> 预测同一个值</li>
<li><span class="math inline">\(for\ m=1...K\ do\)</span>:</li>
<li>   <span class="math inline">\(g_i = -y_iu_i\)</span></li>
<li>   以 <span class="math inline">\(g_i\)</span> 为目标拟合一棵 CART
树，得到样本空间的 <span class="math inline">\(R_{jm}\)</span> 的 <span
class="math inline">\(j\)</span> 个划分</li>
<li>   计算 <span class="math inline">\(\gamma_{jm} =
-\frac{\sum_{\mathbf{x}_i\in R_{jm}} y_iu_i}{\sum_{\mathbf{x}_i\in
R_{jm}}u_i(1-u_i)}\)</span></li>
<li>   更新模型 <span class="math inline">\(F_{m}(\mathbf{x}) =
F_{m-1}(\mathbf{x})+\sum_{j=1}^T\gamma_{jm}I(\mathbf{x}\in
R_{jm})\)</span></li>
</ol>
<h2 id="lambdamart">LambdaMART</h2>
<p>   回顾 LambdaRank 算法，在一个 pair 对的样本 <span
class="math inline">\((i,j)\)</span> (<span class="math inline">\(U_i
&gt; U_j\)</span>)下，<strong>样本</strong> (pair 对) 对文档 (单个doc)
<span class="math inline">\(s_i\)</span> 的偏导为：</p>
<p><span class="math display">\[
  \lambda_{ij} = -\frac{\sigma |\Delta Z_{ij}|}{1+e^{\sigma(s_i - s_j)}}
\]</span></p>
<p>针对单个doc <span class="math inline">\(U_i\)</span> 在全体样本上的
<strong>偏导数之和</strong> 为：</p>
<p><span class="math display">\[
  \lambda_i = \sum_{j:(i,j)\in I}\lambda_{ij} - \sum_{j:(j,i)\in
I}\lambda_{ij}
\]</span></p>
<p>从 <strong>偏导数之和</strong> 反推 “损失函数” <span
class="math inline">\(C\)</span> 为：</p>
<p><span class="math display">\[
  C = \sum_{j:\{i,j\}\in I}\mathbb{I}(U_i &gt; U_j)|\Delta
Z_{ij}|log(1+e^{-\sigma(s_i - s_j)})
\]</span> 我们有：</p>
<p><span class="math display">\[
  \frac{\partial C}{\partial s_i} = \lambda_i =
\sum_{j:\{i,j\}}\mathbb{I}(U_i &gt; U_j)\frac{-\sigma |\Delta
Z_{ij}|}{1+e^{\sigma(s_i - s_j)}}
\]</span></p>
<p>令 <span class="math inline">\(\rho_{ij} = \frac{1}{1+e^{\sigma(s_i -
s_j)}}\)</span>，则有：</p>
<p><span class="math display">\[
\begin{aligned}
  \frac{\partial C}{\partial s_i} &amp; =
\sum_{j:\{i,j\}}-\mathbb{I}(U_i &gt; U_j)\sigma |\Delta Z_{ij}|\rho_{ij}
\\
  \frac{\partial^2 C}{\partial s_j^2} &amp; =
\sum_{j:\{i,j\}}\mathbb{I}(U_i &gt; U_j)\sigma |\Delta
Z_{ij}|\rho_{ij}(1-\rho_{ij})
\end{aligned}
\]</span></p>
<p>有了 <span class="math inline">\(\frac{\partial C}{\partial
s_i}\)</span> 和 <span class="math inline">\(\frac{\partial^2
C}{\partial s_i^2}\)</span></p>
<ul>
<li><p>树可以通过拟合 <span class="math inline">\(\frac{\partial
C}{\partial s_i}\)</span> 得来</p></li>
<li><p>而叶节点的预测值可以通过 <span
class="math inline">\(\frac{\partial C}{\partial s_i}\)</span> 和 <span
class="math inline">\(\frac{\partial^2 C}{\partial s_i^2}\)</span>
来计算。 <span class="math display">\[
\begin{aligned}
  \gamma_{jm} &amp; = \frac{\sum_{\mathbf{x}_i\in R_{jm}} \frac{\partial
C}{\partial s_i}}{\sum_{\mathbf{x}_i\in R_{jm}} \frac{\partial^2
C}{\partial s_i^2}} \\
  &amp;= -\frac{\sum_{\mathbf{x}_i\in R_{jm}}
\sum_{j:\{i,j\}}\mathbb{I}(U_i &gt; U_j)\sigma |\Delta Z_{ij}|\rho_{ij}}
  {\sum_{\mathbf{x}_i\in R_{jm}} \sum_{j:\{i,j\}}\mathbb{I}(U_i &gt;
U_j)\sigma |\Delta Z_{ij}|\rho_{ij}(1-\rho_{ij})}
  \end{aligned}
\]</span></p>
<ul>
<li>这个公式也太复杂了，必须要好好揣摩下。</li>
</ul></li>
</ul>
<p>于是我们有了大名鼎鼎的 LambdaMART 算法：</p>
<p><img src = "/images/LambdaMART.png" width = 480></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GBDT/" rel="tag"># GBDT</a>
          
            <a href="/tags/XGBoost/" rel="tag"># XGBoost</a>
          
            <a href="/tags/LightGBM/" rel="tag"># LightGBM</a>
          
            <a href="/tags/LTR/" rel="tag"># LTR</a>
          
            <a href="/tags/NDCG/" rel="tag"># NDCG</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2022/05/11/DesignPattern/" rel="next" title="设计模式">
                <i class="fa fa-chevron-left"></i> 设计模式
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives%7C%7C%20archive">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zegzag" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#gbdt-%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">GBDT 相关算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#decision-tree"><span class="nav-number">1.1.</span> <span class="nav-text">Decision Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%A0%E5%B0%84%E8%A1%A8%E7%A4%BA"><span class="nav-number">1.1.1.</span> <span class="nav-text">映射表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.2.</span> <span class="nav-text">基础算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#id3-%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.3.</span> <span class="nav-text">ID3 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#c4.5-%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.4.</span> <span class="nav-text">C4.5 算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cart-%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.5.</span> <span class="nav-text">CART 算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gradient-boosting-gb"><span class="nav-number">1.2.</span> <span class="nav-text">Gradient Boosting (GB)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%A0%E5%B0%84%E8%A1%A8%E7%A4%BA-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">映射表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B-gradient-boosting"><span class="nav-number">1.2.2.</span> <span class="nav-text">学习过程 (Gradient Boosting)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gradient-boosting-desicion-tree-gbdt"><span class="nav-number">1.2.3.</span> <span class="nav-text">Gradient Boosting Desicion
Tree (GBDT)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost"><span class="nav-number">1.3.</span> <span class="nav-text">XGBoost</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%90%86%E8%AE%BA"><span class="nav-number">1.3.1.</span> <span class="nav-text">理论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B2%BE%E7%A1%AE%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.2.</span> <span class="nav-text">精确贪心算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E7%AE%97%E6%B3%95-weighted-quantile-sketch"><span class="nav-number">1.3.3.</span> <span class="nav-text">近似算法 (Weighted Quantile
Sketch)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96"><span class="nav-number">1.3.4.</span> <span class="nav-text">其他优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lightgbm"><span class="nav-number">1.4.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#goss"><span class="nav-number">1.4.1.</span> <span class="nav-text">GOSS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#efb"><span class="nav-number">1.4.2.</span> <span class="nav-text">EFB</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leaf-wise-learning"><span class="nav-number">1.4.3.</span> <span class="nav-text">Leaf-wise Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#histogram-substraction"><span class="nav-number">1.4.4.</span> <span class="nav-text">Histogram Substraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#feature-parallel"><span class="nav-number">1.4.5.</span> <span class="nav-text">Feature Parallel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#data-parallel"><span class="nav-number">1.4.6.</span> <span class="nav-text">Data Parallel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#voting-parallel"><span class="nav-number">1.4.7.</span> <span class="nav-text">Voting Parallel</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gbdt-%E5%9C%A8-ranking-%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">GBDT 在 Ranking 领域的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ranknet"><span class="nav-number">2.1.</span> <span class="nav-text">RankNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8A%A0%E9%80%9F"><span class="nav-number">2.1.1.</span> <span class="nav-text">梯度下降法加速</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ir-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">2.2.</span> <span class="nav-text">IR 评价指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lambdarank"><span class="nav-number">2.3.</span> <span class="nav-text">LambdaRank</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#delta-ndcg-%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95"><span class="nav-number">2.3.1.</span> <span class="nav-text">\(\Delta
NDCG\) 计算方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9-lambda_ij-%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">2.3.2.</span> <span class="nav-text">对 \(\lambda_{ij}\) 的改进</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">2.3.3.</span> <span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mart"><span class="nav-number">2.4.</span> <span class="nav-text">MART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB-pm-1-%E9%97%AE%E9%A2%98%E4%B8%8B%E7%9A%84-gbdt-%E5%BD%A2%E5%BC%8F"><span class="nav-number">2.4.1.</span> <span class="nav-text">二分类 \(\pm 1\) 问题下的 GBDT 形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-number">2.4.2.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lambdamart"><span class="nav-number">2.5.</span> <span class="nav-text">LambdaMART</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Z</span>

  
</div>
<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>

-->


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"live2d-widget-model-hibiki"},"display":{"position":"right","width":150,"height":330,"hOffset":50,"vOffset":0},"mobile":{"show":true,"scale":0.5},"react":{"opacity":0.7}});</script></body>
</html>
